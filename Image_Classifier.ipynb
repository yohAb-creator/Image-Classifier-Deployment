{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42194e8b-65a6-479d-a22a-5b9a5a360a45",
      "metadata": {
        "id": "42194e8b-65a6-479d-a22a-5b9a5a360a45"
      },
      "outputs": [],
      "source": [
        "!pip uninstall torch torchvision torchaudio\n",
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ac198b3-0f46-432c-8fa0-45b0646f9d7e",
      "metadata": {
        "id": "0ac198b3-0f46-432c-8fa0-45b0646f9d7e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "from PIL import Image\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "cudnn.benchmark = True\n",
        "plt.ion()   # interactive mode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "F5jKAxuh91u8"
      },
      "id": "F5jKAxuh91u8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7ae088f8-12bc-447f-8307-f4c0bda8435b",
      "metadata": {
        "id": "7ae088f8-12bc-447f-8307-f4c0bda8435b"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e69ede3-b736-4616-b11b-65bf4823f51a",
      "metadata": {
        "id": "5e69ede3-b736-4616-b11b-65bf4823f51a"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# Cell 2: Data Loading & Transformation (CIFAR-10)\n",
        "\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(224), # Resize the tiny 32x32 image to the size ResNet expects\n",
        "        transforms.RandomHorizontalFlip(), # A standard, non-destructive augmentation\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(224), # Just resize for validation, no cropping needed\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                             train=True,\n",
        "                                             download=True,\n",
        "                                             transform=data_transforms['train'])\n",
        "\n",
        "val_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                           train=False,\n",
        "                                           download=True,\n",
        "                                           transform=data_transforms['val'])\n",
        "\n",
        "image_datasets = {'train': train_dataset, 'val': val_dataset}\n",
        "\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "              for x in ['train', 'val']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "# We want to be able to train our model on an `accelerator`\n",
        "# such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU.\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "print(f\"Classes: {class_names}\")\n",
        "\n",
        "print(\"\\nCell 2 Execution Complete: Data pipeline re-engineered for CIFAR-10.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# View Images"
      ],
      "metadata": {
        "id": "PRYDg8mMLSZq"
      },
      "id": "PRYDg8mMLSZq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82be5dfc-e081-44ab-a25c-aa3dc235de85",
      "metadata": {
        "id": "82be5dfc-e081-44ab-a25c-aa3dc235de85"
      },
      "outputs": [],
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Display image for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ],
      "metadata": {
        "id": "lOv4ZwhjLgMI"
      },
      "id": "lOv4ZwhjLgMI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69ac5468-50a9-4a49-a299-217059b4d808",
      "metadata": {
        "id": "69ac5468-50a9-4a49-a299-217059b4d808"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, save_path, num_epochs=25, patience=5, min_delta=0.001):\n",
        "    \"\"\"\n",
        "    Trains a model and saves the best performing weights to a specified path.\n",
        "    Includes an early stopping mechanism.\n",
        "    \"\"\"\n",
        "    since = time.time()\n",
        "\n",
        "    # Ensure the directory for the save_path exists\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "    best_model_params_path = save_path\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # --- Early Stopping Parameters ---\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # --- Early Stopping Logic ---\n",
        "            if phase == 'val':\n",
        "                # Check if the model has improved by at least min_delta\n",
        "                if epoch_acc > best_acc + min_delta:\n",
        "                    best_acc = epoch_acc\n",
        "                    epochs_no_improve = 0  # Reset patience counter\n",
        "                    torch.save(model.state_dict(), best_model_params_path)\n",
        "                    print(f\"New best model saved to {best_model_params_path} with accuracy: {best_acc:.4f}\")\n",
        "                else:\n",
        "                    epochs_no_improve += 1\n",
        "\n",
        "        # Check if training should be stopped\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f\"\\nEarly stopping triggered after {patience} epochs with no improvement.\")\n",
        "            break\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    model.load_state_dict(torch.load(best_model_params_path))\n",
        "    return model\n",
        "\n",
        "print(\"Cell 4 Execution Complete: train_model function is defined with Early Stopping.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualize Model Predictions"
      ],
      "metadata": {
        "id": "r6-wFPhvL3pl"
      },
      "id": "r6-wFPhvL3pl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d6e05a4-892c-4b40-ad9e-8342ecad0962",
      "metadata": {
        "id": "8d6e05a4-892c-4b40-ad9e-8342ecad0962"
      },
      "outputs": [],
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune the ConvNet"
      ],
      "metadata": {
        "id": "e4Y8WdSzMIJw"
      },
      "id": "e4Y8WdSzMIJw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2b3b737-928c-43e6-a273-57a683fb4ec4",
      "metadata": {
        "id": "a2b3b737-928c-43e6-a273-57a683fb4ec4"
      },
      "outputs": [],
      "source": [
        "# --- FINETUNING THE CONVNET FOR CIFAR10 ---\n",
        "\n",
        "\n",
        "# 1. Load the pretrained ResNet18 architecture and weights\n",
        "model_ft = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "# 2. Extract the number of input features for the final layer\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "\n",
        "# 3. Re-engineer the final classification layer for the 10 CIFAR10 classes\n",
        "model_ft.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "\n",
        "# 4. Transfer the model to the appropriate computational substrate (GPU or CPU)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# 5. Define the loss function (criterion)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 6. Define the optimizer, ensuring it targets all parameters of the fine-tuned model\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 7. Define the learning rate scheduler\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "print(\"Model_ft successfully initialized and configured.\")\n",
        "print(f\"Model is on device: {next(model_ft.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and evaluate"
      ],
      "metadata": {
        "id": "Te840C0QMTaL"
      },
      "id": "Te840C0QMTaL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cddd8d13-6a57-427f-ab10-3f1c8332380f",
      "metadata": {
        "id": "cddd8d13-6a57-427f-ab10-3f1c8332380f"
      },
      "outputs": [],
      "source": [
        "# 1a. Define the permanent archival path in your Google Drive for the fine-tuned model.\n",
        "finetuned_model_save_path = \"/content/drive/MyDrive/Colab_Models/cifar10_resnet18_finetuned.pt\"\n",
        "\n",
        "# 1b. Call the training function with 'patience' set to halt the process.\n",
        "print(\"\\n--- INITIATING PROTOCOL 1: FINETUNING (WITH EARLY STOPPING) ---\")\n",
        "model_ft = train_model(model_ft,\n",
        "                       criterion,\n",
        "                       optimizer_ft,\n",
        "                       exp_lr_scheduler,\n",
        "                       save_path=finetuned_model_save_path,\n",
        "                       num_epochs=25,\n",
        "                       patience=5, # <-- Stops unnecessary training\n",
        "                       min_delta=0.001)\n",
        "\n",
        "print(\"\\nProtocol 1 Execution Complete: Fine-tuning finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b4ea91c-77f9-4f8b-a7ad-8e45da01174d",
      "metadata": {
        "id": "1b4ea91c-77f9-4f8b-a7ad-8e45da01174d"
      },
      "outputs": [],
      "source": [
        "visualize_model(model_ft)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConvNet as fixed feature extractor"
      ],
      "metadata": {
        "id": "7HPfOp2DMbhk"
      },
      "id": "7HPfOp2DMbhk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2988108-ec03-4090-9576-1cedbd2acb85",
      "metadata": {
        "id": "a2988108-ec03-4090-9576-1cedbd2acb85"
      },
      "outputs": [],
      "source": [
        "# --- Protocol 2: ConvNet as a Fixed Feature Extractor ---\n",
        "\n",
        "print(\"\\n--- INITIATING PROTOCOL 2: FEATURE EXTRACTION (WITH EARLY STOPPING) ---\")\n",
        "\n",
        "# 2a. Load a fresh pretrained model\n",
        "model_conv = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "# Freeze all the network's parameters\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Re-engineer the final layer for the 10 CIFAR10 classes.\n",
        "# Parameters of newly constructed modules have requires_grad=True by default.\n",
        "num_ftrs = model_conv.fc.in_features\n",
        "model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "\n",
        "# Transfer the model to the appropriate computational substrate\n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "# Define the loss function\n",
        "criterion_conv = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer to ONLY train the new final layer\n",
        "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "exp_lr_scheduler_conv = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Evaluate"
      ],
      "metadata": {
        "id": "WlEhT4NvMoHQ"
      },
      "id": "WlEhT4NvMoHQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f5e3cee-b388-47c0-b763-0d5a59252427",
      "metadata": {
        "id": "8f5e3cee-b388-47c0-b763-0d5a59252427"
      },
      "outputs": [],
      "source": [
        "# 2b. Define the archival path for the feature extractor model.\n",
        "feature_extractor_model_save_path = \"/content/drive/MyDrive/Colab_Models/cifar10_resnet18_feature_extractor.pt\"\n",
        "\n",
        "# 2c. Call the training function for the feature extractor model.\n",
        "model_conv = train_model(model_conv,\n",
        "                         criterion_conv,\n",
        "                         optimizer_conv,\n",
        "                         exp_lr_scheduler_conv,\n",
        "                         save_path=feature_extractor_model_save_path,\n",
        "                         num_epochs=25,\n",
        "                         patience=5, # <-- Stops unnecessary training\n",
        "                         min_delta=0.001)\n",
        "\n",
        "print(\"\\nProtocol 2 Execution Complete: Feature extraction training finished.\")\n",
        "print(\"\\nCell 6 Execution Complete: Both models trained and best versions loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58a41df5-ae7d-45b9-a2a3-e1a3129a9ef4",
      "metadata": {
        "id": "58a41df5-ae7d-45b9-a2a3-e1a3129a9ef4"
      },
      "source": [
        "# Inference on custom images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "842b5cc5-d329-4782-bb7b-0c2b9009aae6",
      "metadata": {
        "id": "842b5cc5-d329-4782-bb7b-0c2b9009aae6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def un_normalize(tensor):\n",
        "    \"\"\"Reverses the normalization on a tensor to make it displayable.\"\"\"\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    # Create a new tensor to avoid modifying the original in place\n",
        "    un_normalized_tensor = tensor.clone()\n",
        "    for t, m, s in zip(un_normalized_tensor, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "    return un_normalized_tensor\n",
        "\n",
        "def visualize_model_prediction_tensor(model, img_tensor, true_label, ax=None):\n",
        "    \"\"\"Visualizes a model's prediction for a single image tensor.\"\"\"\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "    # --- Model Prediction Logic ---\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Add a batch dimension and send to device\n",
        "        outputs = model(img_tensor.unsqueeze(0).to(device))\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        predicted_class = class_names[preds[0]]\n",
        "\n",
        "    # --- Visualization Logic ---\n",
        "    # Un-normalize the image tensor before displaying\n",
        "    display_tensor = un_normalize(img_tensor.cpu())\n",
        "\n",
        "    # Transpose from (C, H, W) to (H, W, C) for matplotlib\n",
        "    ax.imshow(display_tensor.permute(1, 2, 0))\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f'True: {true_label}\\nPredicted: {predicted_class}')\n",
        "\n",
        "    model.train(mode=was_training)\n",
        "    return ax, predicted_class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "919ed37a-0dc6-46d5-9062-d0521a0df191",
      "metadata": {
        "id": "919ed37a-0dc6-46d5-9062-d0521a0df191"
      },
      "outputs": [],
      "source": [
        "# Get a single image and label from the validation set\n",
        "img, label = val_dataset[200]\n",
        "true_class_name = class_names[label]\n",
        "\n",
        "# Call the new function\n",
        "visualize_model_prediction_tensor(model_conv, img, true_class_name)\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C6IkTQBdWjCW"
      },
      "id": "C6IkTQBdWjCW",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}